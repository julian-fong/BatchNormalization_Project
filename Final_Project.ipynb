{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JDm2khf1fZY",
        "outputId": "54bed5c2-0c0d-4bfb-81c1-f4522e1b45bd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAEz59H4hmIH"
      },
      "source": [
        "#FINAL PROJECT\n",
        "#JULIAN FONG AND HAOHUA TANG\n",
        "\n",
        "#Understanding Layers of Batch Normalization through Convolutional Neural Networks\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras import backend as K\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.datasets import cifar10\n",
        "#%cd /content/gdrive/MyDrive/data\n",
        "data_path = \"/content/gdrive/MyDrive/data\"\n",
        "\n",
        "IMAGE_SIZE = 64"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4QDQL4HjIc2"
      },
      "source": [
        "#CODE WAS REFERENCED FROM CODEBASE IN ORDER TO CONSTRUCT FACE DATASET. REFERENCE LINK: https://github.com/Hironsan/BossSensor\n",
        "def resize_with_pad(image, height=IMAGE_SIZE, width=IMAGE_SIZE):\n",
        "\n",
        "    def get_padding_size(image):\n",
        "        h, w, _ = image.shape\n",
        "        longest_edge = max(h, w)\n",
        "        top, bottom, left, right = (0, 0, 0, 0)\n",
        "        if h < longest_edge:\n",
        "            dh = longest_edge - h\n",
        "            top = dh // 2\n",
        "            bottom = dh - top\n",
        "        elif w < longest_edge:\n",
        "            dw = longest_edge - w\n",
        "            left = dw // 2\n",
        "            right = dw - left\n",
        "        else:\n",
        "            pass\n",
        "        return top, bottom, left, right\n",
        "\n",
        "    top, bottom, left, right = get_padding_size(image)\n",
        "    BLACK = [0, 0, 0]\n",
        "    constant = cv2.copyMakeBorder(image, top , bottom, left, right, cv2.BORDER_CONSTANT, value=BLACK)\n",
        "\n",
        "    resized_image = cv2.resize(constant, (height, width))\n",
        "\n",
        "    return resized_image\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIGdrUM5jLqs"
      },
      "source": [
        "##CODE WAS REFERENCED FROM CODEBASE IN ORDER TO CONSTRUCT FACE DATASET. REFERENCE LINK: https://github.com/Hironsan/BossSensor\n",
        "def traverse_dir(path, images, labels):\n",
        "    for file_or_dir in os.listdir(path):\n",
        "        abs_path = os.path.abspath(os.path.join(path, file_or_dir))\n",
        "        print(abs_path)\n",
        "        if os.path.isdir(abs_path):  # dir\n",
        "            traverse_dir(abs_path, images, labels)\n",
        "        else:                        # file\n",
        "            if file_or_dir.endswith('.jpg'):\n",
        "                image = read_image(abs_path)\n",
        "                images.append(image)\n",
        "                labels.append(path)\n",
        "    return images, labels"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zXBNV07jPMI"
      },
      "source": [
        "##CODE WAS REFERENCED FROM CODEBASE IN ORDER TO CONSTRUCT FACE DATASET. REFERENCE LINK: https://github.com/Hironsan/BossSensor\n",
        "def read_image(file_path):\n",
        "    image = cv2.imread(file_path)\n",
        "    image = resize_with_pad(image, IMAGE_SIZE, IMAGE_SIZE)\n",
        "\n",
        "    return image"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXqO2ZHqjQ1d"
      },
      "source": [
        "##CODE WAS REFERENCED FROM CODEBASE IN ORDER TO CONSTRUCT FACE DATASET. REFERENCE LINK: https://github.com/Hironsan/BossSensor\n",
        "def extract_data(path, images, labels):\n",
        "    images, labels = traverse_dir(path, images, labels)\n",
        "    images = np.array(images)\n",
        "    int_labels = np.zeros(len(images))\n",
        "    num_classes = 0\n",
        "    for i in range(1, len(labels)):\n",
        "        if labels[i][-2:] != labels[i-1][-2:]:\n",
        "            num_classes += 1\n",
        "        int_labels[i] = num_classes\n",
        "\n",
        "    return images, int_labels"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Qtst6y9jS2P"
      },
      "source": [
        "##CODE WAS REFERENCED FROM CODEBASE IN ORDER TO CONSTRUCT FACE DATASET. REFERENCE LINK: https://github.com/Hironsan/BossSensor\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "\n",
        "def load_split_data(images, labels, nb_classes=3):\n",
        "    images, labels = extract_data(data_path, images, labels)\n",
        "    labels = np.reshape(labels, [-1])\n",
        "    img_rows, img_cols = IMAGE_SIZE, IMAGE_SIZE\n",
        "    X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=1)\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
        "\n",
        "    if K.image_data_format() == 'channels first':\n",
        "        X_train = X_train.reshape(X_train.shape[0], 3, img_rows, img_cols)\n",
        "        X_val = X_val.reshape(X_val.shape[0], 3, img_rows, img_cols)\n",
        "        X_test = X_test.reshape(X_test.shape[0], 3, img_rows, img_cols)\n",
        "        input_shape = (3, img_rows, img_cols)\n",
        "    else:\n",
        "        X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 3)\n",
        "        X_val = X_val.reshape(X_val.shape[0], img_rows, img_cols, 3)\n",
        "        X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 3)\n",
        "        input_shape = (img_rows, img_cols, 3)\n",
        "\n",
        "    print('X_train shape:', X_train.shape)\n",
        "    print(X_train.shape[0], 'train samples')\n",
        "    print(X_val.shape[0], 'valid samples')\n",
        "    print(X_test.shape[0], 'test samples')\n",
        "\n",
        "    y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "    y_val = np_utils.to_categorical(y_val, nb_classes)\n",
        "    y_test = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_val = X_val.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "    X_train /= 255\n",
        "    X_val /= 255\n",
        "    X_test /= 255\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, input_shape"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqLK_IeZjkaf"
      },
      "source": [
        "X_train, X_val, X_test, y_train, y_val, y_test, input_shape = load_split_data(images, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGVYobtXpQqn",
        "outputId": "4cd92ed0-682b-4eb9-ff0f-1e3c0f4fc799"
      },
      "source": [
        "(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "\n",
        "trainY = np_utils.to_categorical(trainY)\n",
        "testY = np_utils.to_categorical(testY)\n",
        "\n",
        "trainX = trainX.astype('float32')\n",
        "trainY = trainY.astype('float32')\n",
        "testX = testX.astype('float32')\n",
        "testY = testY.astype('float32')\n",
        "\n",
        "trainX /= 255\n",
        "testX /= 255"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0ZlDi1eonzR",
        "outputId": "2c31b030-9224-4d5e-b6b8-2cb2477e3476"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(trainX.shape)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1801, 64, 64, 3)\n",
            "(50000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4izkOGinM6z"
      },
      "source": [
        "#CODE REFERENCED FROM KERAS TUTORIALS. CODE REFERENCED LINKS: https://keras.io/api/models/model_training_apis/ , https://keras.io/api/preprocessing/image/, https://appliedmachinelearning.blog/2018/03/24/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks/\n",
        "#SPECIFICALLY FOR THE TRAIN FUNCTIONS AND THE IMAGE GENERATORS\n",
        "#CODE FOR CONVOLUTIONAL MODEL WAS CHANGED TO REFLECT GOALS AND AIMS FOR TASK.\n",
        "class conv_net(object):\n",
        "    def __init__(self, input_shape, use_batchnorm=True, nb_classes = 3, lr=0.01, BN_selection = [False, False, False, False]):\n",
        "        self.use_batchnorm = use_batchnorm\n",
        "        self.lr = lr\n",
        "        self.model = Sequential()\n",
        "\n",
        "        self.model.add(Conv2D(64, (3, 3), padding='same', input_shape=input_shape))\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.model.add(MaxPooling2D(pool_size=(2, 2), padding = 'same'))\n",
        "        if use_batchnorm and BN_selection[0]:\n",
        "            self.model.add(BatchNormalization())\n",
        "        self.model.add(Conv2D(64, (3, 3)))\n",
        "        self.model.add(Activation('relu'))\n",
        "        if use_batchnorm and BN_selection[1]:\n",
        "            self.model.add(BatchNormalization())\n",
        "        self.model.add(MaxPooling2D(pool_size=(2, 2), padding = 'same'))\n",
        "\n",
        "        self.model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.model.add(MaxPooling2D(pool_size=(2, 2), padding = 'same'))\n",
        "        if use_batchnorm and BN_selection[2]:\n",
        "            self.model.add(BatchNormalization())\n",
        "        self.model.add(Conv2D(128, (3, 3)))\n",
        "        self.model.add(Activation('relu'))\n",
        "        if use_batchnorm and BN_selection[3]:\n",
        "            self.model.add(BatchNormalization())\n",
        "        self.model.add(MaxPooling2D(pool_size=(2, 2), padding = 'same'))\n",
        "\n",
        "        self.model.add(Flatten())\n",
        "        self.model.add(Dense(512))\n",
        "        self.model.add(Activation('relu'))\n",
        "        self.model.add(Dense(nb_classes))\n",
        "        self.model.add(Activation('softmax'))\n",
        "\n",
        "    def train(self, X_train, X_val, y_train, y_val, batch_size=32, nb_epoch=40, data_augmentation=True):\n",
        "        sgd = SGD(lr=self.lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "        self.model.compile(loss='categorical_crossentropy',\n",
        "                           optimizer=sgd,\n",
        "                           metrics=['accuracy'])\n",
        "        if not data_augmentation:\n",
        "            print('Not using data augmentation.')\n",
        "            self.model.fit(X_train, y_train,\n",
        "                           batch_size=batch_size,\n",
        "                           nb_epoch=nb_epoch,\n",
        "                           validation_data=(X_val, y_val),\n",
        "                           shuffle=True)\n",
        "        else:\n",
        "            print('Using real-time data augmentation.')\n",
        "\n",
        "            datagen = ImageDataGenerator(\n",
        "                featurewise_center=False,  \n",
        "                samplewise_center=False,  \n",
        "                featurewise_std_normalization=False,  \n",
        "                samplewise_std_normalization=False,  \n",
        "                zca_whitening=False,  \n",
        "                rotation_range=20, \n",
        "                width_shift_range=0.2,  \n",
        "                height_shift_range=0.2,  \n",
        "                horizontal_flip=True,  \n",
        "                vertical_flip=False)  \n",
        "\n",
        "\n",
        "            datagen.fit(X_train)\n",
        "\n",
        "            self.model.fit(datagen.flow(X_train, y_train,\n",
        "                                                  batch_size=batch_size),\n",
        "                                     steps_per_epoch=(X_train.shape[0])//batch_size,\n",
        "                                     epochs=nb_epoch,\n",
        "                                     validation_data=(X_val, y_val))\n",
        "\n",
        "\n",
        "    def train2(self, trainX, trainY,batch_size=32, nb_epoch=40, data_augmentation=True):\n",
        "        sgd = SGD(lr=self.lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "        self.model.compile(loss='categorical_crossentropy',\n",
        "                           optimizer=sgd,\n",
        "                           metrics=['accuracy'])\n",
        "        if not data_augmentation:\n",
        "            print('Not using data augmentation.')\n",
        "            self.model.fit(trainX, trainY,\n",
        "                           batch_size=batch_size,\n",
        "                           epochs=nb_epoch,\n",
        "                           validation_data=None,\n",
        "                           shuffle=True)\n",
        "        else:\n",
        "            print('Using real-time data augmentation.')\n",
        "\n",
        "            datagen = ImageDataGenerator(\n",
        "                featurewise_center=False,  \n",
        "                samplewise_center=False,  \n",
        "                featurewise_std_normalization=False,  \n",
        "                samplewise_std_normalization=False,  \n",
        "                zca_whitening=False,  \n",
        "                rotation_range=20,  \n",
        "                width_shift_range=0.2,  \n",
        "                height_shift_range=0.2,  \n",
        "                horizontal_flip=True,  \n",
        "                vertical_flip=False)  \n",
        "\n",
        "\n",
        "            datagen.fit(trainX)\n",
        "\n",
        "            self.model.fit(datagen.flow(trainX, trainY,batch_size=batch_size),steps_per_epoch=(trainX.shape[0])//batch_size,epochs=nb_epoch)\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        score = self.model.evaluate(X_test, y_test, verbose=0)\n",
        "        print(\"%s: %.2f%%\" % (self.model.metrics_names[1], score[1] * 100))\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ytgOyaJnVA_",
        "outputId": "16d5eb99-3336-49ec-9334-a4d5b679a976"
      },
      "source": [
        "model = conv_net(input_shape, nb_classes = 3)\n",
        "model.train(X_train, X_val, y_train, y_val)\n",
        "model.evaluate(X_test, y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "input_size = (32, 32, 3)\n",
        "model2 = conv_net(input_size, nb_classes = 10)\n",
        "model2.train2(trainX, trainY)\n",
        "model2.evaluate(testX, testY)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "Epoch 1/40\n",
            "56/56 [==============================] - 23s 400ms/step - loss: 1.0268 - accuracy: 0.4656 - val_loss: 0.2940 - val_accuracy: 0.9168\n",
            "Epoch 2/40\n",
            "56/56 [==============================] - 22s 392ms/step - loss: 0.4650 - accuracy: 0.8303 - val_loss: 0.2245 - val_accuracy: 0.9468\n",
            "Epoch 3/40\n",
            "56/56 [==============================] - 22s 394ms/step - loss: 0.2449 - accuracy: 0.9229 - val_loss: 0.0698 - val_accuracy: 0.9834\n",
            "Epoch 4/40\n",
            "56/56 [==============================] - 22s 394ms/step - loss: 0.1286 - accuracy: 0.9593 - val_loss: 0.0472 - val_accuracy: 0.9884\n",
            "Epoch 5/40\n",
            "56/56 [==============================] - 22s 393ms/step - loss: 0.1418 - accuracy: 0.9591 - val_loss: 0.0606 - val_accuracy: 0.9834\n",
            "Epoch 6/40\n",
            "56/56 [==============================] - 22s 394ms/step - loss: 0.1105 - accuracy: 0.9670 - val_loss: 0.0360 - val_accuracy: 0.9900\n",
            "Epoch 7/40\n",
            "56/56 [==============================] - 22s 393ms/step - loss: 0.0799 - accuracy: 0.9667 - val_loss: 0.0429 - val_accuracy: 0.9917\n",
            "Epoch 8/40\n",
            "56/56 [==============================] - 22s 393ms/step - loss: 0.0587 - accuracy: 0.9817 - val_loss: 0.0379 - val_accuracy: 0.9933\n",
            "Epoch 9/40\n",
            "56/56 [==============================] - 22s 393ms/step - loss: 0.0519 - accuracy: 0.9798 - val_loss: 0.0351 - val_accuracy: 0.9933\n",
            "Epoch 10/40\n",
            "56/56 [==============================] - 22s 392ms/step - loss: 0.0460 - accuracy: 0.9876 - val_loss: 0.0253 - val_accuracy: 0.9950\n",
            "Epoch 11/40\n",
            "56/56 [==============================] - 22s 393ms/step - loss: 0.0461 - accuracy: 0.9863 - val_loss: 0.0305 - val_accuracy: 0.9950\n",
            "Epoch 12/40\n",
            "56/56 [==============================] - 22s 393ms/step - loss: 0.0478 - accuracy: 0.9816 - val_loss: 0.0406 - val_accuracy: 0.9933\n",
            "Epoch 13/40\n",
            "56/56 [==============================] - 22s 392ms/step - loss: 0.0250 - accuracy: 0.9908 - val_loss: 0.0454 - val_accuracy: 0.9817\n",
            "Epoch 14/40\n",
            "56/56 [==============================] - 22s 392ms/step - loss: 0.0478 - accuracy: 0.9904 - val_loss: 0.0294 - val_accuracy: 0.9917\n",
            "Epoch 15/40\n",
            "56/56 [==============================] - 22s 392ms/step - loss: 0.0648 - accuracy: 0.9792 - val_loss: 0.0777 - val_accuracy: 0.9717\n",
            "Epoch 16/40\n",
            "56/56 [==============================] - 22s 397ms/step - loss: 0.0735 - accuracy: 0.9719 - val_loss: 0.0328 - val_accuracy: 0.9884\n",
            "Epoch 17/40\n",
            "56/56 [==============================] - 22s 392ms/step - loss: 0.0395 - accuracy: 0.9887 - val_loss: 0.0509 - val_accuracy: 0.9800\n",
            "Epoch 18/40\n",
            "56/56 [==============================] - 22s 391ms/step - loss: 0.0252 - accuracy: 0.9922 - val_loss: 0.0387 - val_accuracy: 0.9917\n",
            "Epoch 19/40\n",
            "56/56 [==============================] - 22s 391ms/step - loss: 0.0250 - accuracy: 0.9905 - val_loss: 0.0165 - val_accuracy: 0.9933\n",
            "Epoch 20/40\n",
            "56/56 [==============================] - 22s 393ms/step - loss: 0.0160 - accuracy: 0.9937 - val_loss: 0.0399 - val_accuracy: 0.9867\n",
            "Epoch 21/40\n",
            "56/56 [==============================] - 22s 393ms/step - loss: 0.0180 - accuracy: 0.9956 - val_loss: 0.0149 - val_accuracy: 0.9933\n",
            "Epoch 22/40\n",
            "56/56 [==============================] - 22s 392ms/step - loss: 0.0309 - accuracy: 0.9874 - val_loss: 0.0277 - val_accuracy: 0.9933\n",
            "Epoch 23/40\n",
            "56/56 [==============================] - 22s 393ms/step - loss: 0.0247 - accuracy: 0.9918 - val_loss: 0.0699 - val_accuracy: 0.9884\n",
            "Epoch 24/40\n",
            "56/56 [==============================] - 22s 387ms/step - loss: 2.0983 - accuracy: 0.6678 - val_loss: 0.0755 - val_accuracy: 0.9800\n",
            "Epoch 25/40\n",
            "56/56 [==============================] - 22s 389ms/step - loss: 0.1717 - accuracy: 0.9401 - val_loss: 0.0486 - val_accuracy: 0.9867\n",
            "Epoch 26/40\n",
            "56/56 [==============================] - 22s 388ms/step - loss: 0.1166 - accuracy: 0.9608 - val_loss: 0.0377 - val_accuracy: 0.9900\n",
            "Epoch 27/40\n",
            "56/56 [==============================] - 22s 385ms/step - loss: 0.0924 - accuracy: 0.9669 - val_loss: 0.0327 - val_accuracy: 0.9917\n",
            "Epoch 28/40\n",
            "56/56 [==============================] - 22s 386ms/step - loss: 0.0526 - accuracy: 0.9830 - val_loss: 0.0275 - val_accuracy: 0.9933\n",
            "Epoch 29/40\n",
            "56/56 [==============================] - 22s 385ms/step - loss: 0.0966 - accuracy: 0.9694 - val_loss: 0.0267 - val_accuracy: 0.9933\n",
            "Epoch 30/40\n",
            "56/56 [==============================] - 22s 385ms/step - loss: 0.0484 - accuracy: 0.9851 - val_loss: 0.0191 - val_accuracy: 0.9950\n",
            "Epoch 31/40\n",
            "56/56 [==============================] - 22s 387ms/step - loss: 0.0656 - accuracy: 0.9809 - val_loss: 0.0257 - val_accuracy: 0.9900\n",
            "Epoch 32/40\n",
            "56/56 [==============================] - 22s 386ms/step - loss: 0.0429 - accuracy: 0.9870 - val_loss: 0.0242 - val_accuracy: 0.9917\n",
            "Epoch 33/40\n",
            "56/56 [==============================] - 22s 385ms/step - loss: 0.0799 - accuracy: 0.9782 - val_loss: 0.0196 - val_accuracy: 0.9950\n",
            "Epoch 34/40\n",
            "56/56 [==============================] - 22s 386ms/step - loss: 0.0550 - accuracy: 0.9854 - val_loss: 0.0185 - val_accuracy: 0.9933\n",
            "Epoch 35/40\n",
            "56/56 [==============================] - 22s 390ms/step - loss: 0.0595 - accuracy: 0.9759 - val_loss: 0.0235 - val_accuracy: 0.9933\n",
            "Epoch 36/40\n",
            "56/56 [==============================] - 22s 385ms/step - loss: 0.0379 - accuracy: 0.9876 - val_loss: 0.0152 - val_accuracy: 0.9950\n",
            "Epoch 37/40\n",
            "56/56 [==============================] - 22s 384ms/step - loss: 0.0435 - accuracy: 0.9882 - val_loss: 0.0163 - val_accuracy: 0.9933\n",
            "Epoch 38/40\n",
            "56/56 [==============================] - 22s 384ms/step - loss: 0.0417 - accuracy: 0.9887 - val_loss: 0.0164 - val_accuracy: 0.9933\n",
            "Epoch 39/40\n",
            "56/56 [==============================] - 22s 385ms/step - loss: 0.0215 - accuracy: 0.9909 - val_loss: 0.0249 - val_accuracy: 0.9884\n",
            "Epoch 40/40\n",
            "56/56 [==============================] - 22s 386ms/step - loss: 0.0212 - accuracy: 0.9950 - val_loss: 0.0116 - val_accuracy: 0.9950\n",
            "accuracy: 99.33%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}